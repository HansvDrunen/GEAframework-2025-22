{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Web Scraper & Page Scorer for Mission‚ÄìVision Discovery\n",
        "\n",
        "This notebook cell automatically visits a list of company websites and extracts meaningful textual pages such as Mission, Vision, Values, Strategy, Sustainability, or Annual Reports.\n",
        "It converts each page into a Markdown file for easy reading, and it also ranks them based on how relevant they are to the company‚Äôs purpose and values."
      ],
      "metadata": {
        "id": "WzVOJwlCouRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ **Batch Web Scraper**\n",
        "\n",
        "This part of the code is designed to collect and save web pages from a list of URLs (or from a CSV file).\n",
        "It focuses mainly on fetching and cleaning content, not ranking it yet.\n",
        "\n",
        "What it does:\n",
        "\n",
        "* Reads URLs either directly from the code or from a file called candidates.csv.\n",
        "\n",
        "* Visits each website one by one while respecting the site‚Äôs robots.txt (the file that says what bots are allowed to access).\n",
        "\n",
        "* Downloads each page‚Äôs HTML content and removes irrelevant parts such as navigation bars, headers, and cookie banners.\n",
        "\n",
        "* Extracts only the main readable content ‚Äî titles, headings, and paragraphs.\n",
        "\n",
        "* Converts that clean text into a Markdown file (.md) and saves it locally in a folder (out_md/).\n",
        "\n",
        "* Creates a manifest.csv file with basic information about each page, such as:\n",
        "\n",
        "\n",
        "    *  URL\n",
        "    *  Status (success, blocked, or error)\n",
        "    *  Number of text blocks found\n",
        "    *  A short text snippet as preview\n",
        "\n",
        "üëâ Purpose: This code prepares clean, structured text files from multiple websites so you can later analyze or process them (for example, extract ‚Äúmission‚Äù or ‚Äúvision‚Äù statements)."
      ],
      "metadata": {
        "id": "Lg-FKLGaxf_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîé Discovery Only (Resilient++) ‚Äî expand hub pages to find real .htm targets\n",
        "!pip -q install duckduckgo_search==6.3.5 tldextract==5.1.2 beautifulsoup4==4.12.3 lxml==5.3.0 pandas==2.2.2\n",
        "\n",
        "import re, time, csv, random\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, urljoin, urlsplit, urlunsplit\n",
        "from urllib import robotparser\n",
        "import requests\n",
        "from duckduckgo_search import DDGS\n",
        "import tldextract\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ========= ‚úèÔ∏è EDIT THESE =========\n",
        "COMPANY_NAME = \"ING Group\"\n",
        "OVERRIDE_HOMEPAGE = \"https://www.ing.com\"\n",
        "MAX_CANDIDATES = 60\n",
        "TOP_N_LIVE = 17\n",
        "EXPAND_FROM_HUBS = True          # fetch hub pages (/about-us, /sustainability, etc.) and mine links\n",
        "MAX_HUBS_TO_EXPAND = 5           # safety cap\n",
        "MAX_LINKS_PER_HUB = 120          # parse limit per hub page\n",
        "# ================================\n",
        "\n",
        "UA = (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "      \"(KHTML, like Gecko) Chrome/124.0 Safari/537.36 (+discovery-only)\")\n",
        "TIMEOUT = 20\n",
        "\n",
        "KEYWORDS_URL = [\n",
        "    \"mission\",\"vision\",\"purpose\",\"values\",\"about\",\"who-we-are\",\"our-company\",\n",
        "    \"strategy\",\"culture\",\"sustainability\",\"corporate-governance\",\"what-we-stand-for\",\n",
        "    \"purpose-and-values\",\"purpose-values\",\"at-a-glance\",\"principles\",\"code-of-conduct\"\n",
        "]\n",
        "CANDIDATE_PATHS = [\n",
        "    \"/\", \"/about\", \"/about-us\", \"/company\", \"/who-we-are\", \"/our-company\",\n",
        "    \"/values\", \"/purpose\", \"/mission\", \"/vision\", \"/about/mission\", \"/about/vision\",\n",
        "    \"/sustainability\", \"/culture\", \"/our-values\", \"/strategy\", \"/purpose-and-values\"\n",
        "]\n",
        "\n",
        "def is_probably_official(domain: str, company_name: str) -> bool:\n",
        "    bad = {\"linkedin.com\",\"facebook.com\",\"instagram.com\",\"x.com\",\"twitter.com\",\n",
        "           \"wikipedia.org\",\"crunchbase.com\",\"glassdoor.com\",\"bloomberg.com\",\n",
        "           \"reuters.com\",\"yahoo.com\",\"google.com\",\"news.google.com\",\"youtube.com\"}\n",
        "    ext = tldextract.extract(domain)\n",
        "    root = f\"{ext.domain}.{ext.suffix}\" if ext.suffix else ext.domain\n",
        "    if root in bad: return False\n",
        "    tokens = re.findall(r\"[a-z0-9]+\", company_name.lower())\n",
        "    hits = sum(tok in ext.domain.lower() for tok in tokens if len(tok) >= 3)\n",
        "    return hits >= 1\n",
        "\n",
        "def normalize_home(url: str) -> str:\n",
        "    url = url.strip().rstrip(\"/\")\n",
        "    if not url.startswith(\"http\"):\n",
        "        url = \"https://\" + url\n",
        "    return url\n",
        "\n",
        "def guess_homepages(company_name: str):\n",
        "    brand = re.sub(r\"[^a-z0-9]+\", \"\", company_name.lower())\n",
        "    if brand in (\"inggroup\",\"ing\",\"ingnv\"):\n",
        "        return [\"https://www.ing.com\",\"https://www.ing.nl\",\"https://www.ing.com.tr\"]\n",
        "    bases = [\n",
        "        \"{b}.com\",\"{b}group.com\",\"{b}-group.com\",\"{b}corp.com\",\"{b}corporate.com\",\n",
        "        \"{b}.co\",\"{b}.io\",\"{b}.net\"\n",
        "    ]\n",
        "    return [f\"https://{t.format(b=brand)}\" for t in bases]\n",
        "\n",
        "def ddg_with_backoff(query, max_results=10, attempts=3):\n",
        "    delay = 1.5\n",
        "    for _ in range(attempts):\n",
        "        try:\n",
        "            with DDGS(timeout=TIMEOUT) as ddgs:\n",
        "                results = list(ddgs.text(query, max_results=max_results, region=\"wt-wt\", safesearch=\"off\"))\n",
        "                if results:\n",
        "                    return results\n",
        "        except Exception:\n",
        "            time.sleep(delay + random.random()); delay *= 2\n",
        "    return []\n",
        "\n",
        "def find_official_homepage(company_name: str) -> str|None:\n",
        "    if OVERRIDE_HOMEPAGE.strip():\n",
        "        return normalize_home(OVERRIDE_HOMEPAGE)\n",
        "    for g in guess_homepages(company_name):\n",
        "        try:\n",
        "            r = requests.head(g, timeout=8, allow_redirects=True)\n",
        "            if r.status_code < 400 and is_probably_official(r.url, company_name):\n",
        "                return normalize_home(r.url)\n",
        "        except: pass\n",
        "    results = ddg_with_backoff(f\"{company_name} official site\", max_results=10, attempts=3)\n",
        "    for r in results:\n",
        "        url = r.get(\"href\") or r.get(\"url\")\n",
        "        if not url: continue\n",
        "        p = urlparse(url)\n",
        "        if p.scheme.startswith(\"http\"):\n",
        "            base = f\"{p.scheme}://{p.netloc}\"\n",
        "            if is_probably_official(base, company_name):\n",
        "                return base.rstrip(\"/\")\n",
        "    return None\n",
        "\n",
        "def allowed_by_robots(url: str) -> bool:\n",
        "    try:\n",
        "        p = urlparse(url); base = f\"{p.scheme}://{p.netloc}\"\n",
        "        rp = robotparser.RobotFileParser()\n",
        "        rp.set_url(urljoin(base, \"/robots.txt\")); rp.read()\n",
        "        return rp.can_fetch(UA, url)\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "def fetch(url: str):\n",
        "    return requests.get(url, headers={\"User-Agent\": UA, \"Accept-Language\": \"en\"},\n",
        "                        timeout=TIMEOUT, allow_redirects=True)\n",
        "\n",
        "# ---------- Hygiene helpers ----------\n",
        "def cleanup_url(u: str) -> str:\n",
        "    \"\"\"Normalize scheme & drop query/fragment, preserving the path exactly (incl. .htm).\"\"\"\n",
        "    if not u: return u\n",
        "    u = u.strip()\n",
        "    if not u: return u\n",
        "    if not u.startswith((\"http://\",\"https://\")):\n",
        "        u = \"https://\" + u\n",
        "    p = urlsplit(u)\n",
        "    return urlunsplit((p.scheme, p.netloc, p.path, \"\", \"\"))\n",
        "\n",
        "def ensure_htm(url: str) -> str:\n",
        "    \"\"\"\n",
        "    ING pages under /About-us/ often end with .htm. Only auto-add when:\n",
        "    - host ends with ing.com\n",
        "    - path (case-insensitive) contains '/about-us/'\n",
        "    - last segment looks like a file (has letters/dashes) and has no '.'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        p = urlparse(url)\n",
        "        path_lower = p.path.lower()\n",
        "        if p.netloc.endswith(\"ing.com\") and \"/about-us/\" in path_lower:\n",
        "            last = p.path.rstrip(\"/\").split(\"/\")[-1]\n",
        "            if last and (\".\" not in last) and re.search(r\"[a-zA-Z\\-]\", last):\n",
        "                return url + \".htm\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return url\n",
        "\n",
        "def preflight(url: str) -> tuple[bool,str,int]:\n",
        "    \"\"\"Validate a URL is fetchable. Try HEAD (redirects ok), fall back to GET on 403/405.\"\"\"\n",
        "    try:\n",
        "        r = requests.head(url, headers={\"User-Agent\": UA}, allow_redirects=True, timeout=12)\n",
        "        if 200 <= r.status_code < 300:\n",
        "            return True, r.url, r.status_code\n",
        "        if r.status_code in (403, 405):\n",
        "            rg = requests.get(url, headers={\"User-Agent\": UA}, allow_redirects=True, timeout=12)\n",
        "            return (200 <= rg.status_code < 300), rg.url, rg.status_code\n",
        "        return False, getattr(r, \"url\", url), r.status_code\n",
        "    except Exception:\n",
        "        try:\n",
        "            rg = requests.get(url, headers={\"User-Agent\": UA}, allow_redirects=True, timeout=12)\n",
        "            return (200 <= rg.status_code < 300), rg.url, rg.status_code\n",
        "        except Exception:\n",
        "            return False, url, 0\n",
        "\n",
        "# ---------- Discovery ----------\n",
        "def discover_candidates(base: str, max_candidates=60):\n",
        "    seen, cands = set(), []\n",
        "\n",
        "    def add(u, reason, boost=0.0):\n",
        "        u = u.split(\"#\")[0].rstrip(\"/\")\n",
        "        if urlparse(u).netloc != urlparse(base).netloc: return\n",
        "        if u in seen: return\n",
        "        seen.add(u)\n",
        "        cands.append({\"url\": u, \"reason\": reason, \"boost\": boost})\n",
        "\n",
        "    # 1) Common paths (hubs)\n",
        "    for p in CANDIDATE_PATHS:\n",
        "        add(urljoin(base, p), \"seed\", 0.2 if p != \"/\" else 0.0)\n",
        "\n",
        "    # 2) Sitemaps (index + children)\n",
        "    for sm in [\"/sitemap.xml\", \"/sitemap_index.xml\", \"/sitemap-index.xml\"]:\n",
        "        sm_url = urljoin(base, sm)\n",
        "        if not allowed_by_robots(sm_url): continue\n",
        "        try:\n",
        "            r = fetch(sm_url)\n",
        "            if not (r.ok and \"xml\" in (r.headers.get(\"content-type\",\"\"))): continue\n",
        "            soup = BeautifulSoup(r.text, \"xml\")\n",
        "            locs = [loc.get_text(strip=True) for loc in soup.find_all(\"loc\")]\n",
        "            # follow a few child sitemaps\n",
        "            child_maps = [u for u in locs if u.endswith(\".xml\")]\n",
        "            for cm in child_maps[:10]:\n",
        "                try:\n",
        "                    rr = fetch(cm)\n",
        "                    if rr.ok and \"xml\" in (rr.headers.get(\"content-type\",\"\")):\n",
        "                        s2 = BeautifulSoup(rr.text, \"xml\")\n",
        "                        locs.extend([loc.get_text(strip=True) for loc in s2.find_all(\"loc\")])\n",
        "                except: pass\n",
        "            for u in locs:\n",
        "                if any(k in u.lower() for k in KEYWORDS_URL):\n",
        "                    add(u, \"sitemap\", 0.9)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # 3) DuckDuckGo site search (best-effort)\n",
        "    try:\n",
        "        with DDGS(timeout=TIMEOUT) as ddgs:\n",
        "            query = f\"site:{urlparse(base).netloc} \" + \" \".join(KEYWORDS_URL[:6])\n",
        "            for r in ddgs.text(query, max_results=25, region=\"wt-wt\", safesearch=\"off\"):\n",
        "                u = r.get(\"href\") or r.get(\"url\")\n",
        "                if u: add(u, \"site_search\", 1.0)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return cands[:max_candidates]\n",
        "\n",
        "# ---------- Expand hubs by parsing page links ----------\n",
        "def is_hub_path(path: str) -> bool:\n",
        "    pl = path.lower()\n",
        "    return any(pl == x or pl.startswith(x + \"/\") for x in [\"/about-us\", \"/sustainability\", \"/about\", \"/company\", \"/who-we-are\", \"/our-company\"])\n",
        "\n",
        "def extract_samehost_links(base_url: str, html: str, limit=120):\n",
        "    base_host = urlparse(base_url).netloc\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    out = []\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if not href or href.startswith(\"#\"):\n",
        "            continue\n",
        "        absu = urljoin(base_url, href)\n",
        "        pu = urlparse(absu)\n",
        "        if pu.netloc != base_host:\n",
        "            continue\n",
        "        # keep keywordy links\n",
        "        low = absu.lower()\n",
        "        if any(k in low for k in KEYWORDS_URL):\n",
        "            out.append(absu.split(\"#\")[0])\n",
        "        if len(out) >= limit:\n",
        "            break\n",
        "    # de-dupe preserving order\n",
        "    seen, ded = set(), []\n",
        "    for u in out:\n",
        "        if u not in seen:\n",
        "            seen.add(u); ded.append(u)\n",
        "    return ded\n",
        "\n",
        "def score(url: str, reason: str, boost: float) -> float:\n",
        "    path = urlparse(url).path\n",
        "    low = path.lower()\n",
        "    s = 0.0\n",
        "    for k in KEYWORDS_URL:\n",
        "        if k in low: s += 1.25\n",
        "    s += boost\n",
        "    # prefer .htm content pages\n",
        "    if low.endswith(\".htm\"): s += 0.8\n",
        "    # shorter-ish paths preferred a bit (but not too strong)\n",
        "    s += max(0, 2.0 - 0.12*len(path))\n",
        "    # shallow depth slight bonus\n",
        "    if path.count(\"/\") <= 2: s += 0.3\n",
        "    # explicit sections bonus\n",
        "    if any(x in low for x in [\"/about\",\"/purpose\",\"/values\",\"/mission\",\"/vision\",\"/strategy\",\"/sustainability\"]): s += 0.5\n",
        "    if reason == \"site_search\": s += 0.3\n",
        "    if reason == \"page_links\": s += 0.6   # links mined from relevant hubs are often good\n",
        "    return round(s, 3)\n",
        "\n",
        "# ---------- Run (discovery-only with expansion & validation) ----------\n",
        "home = find_official_homepage(COMPANY_NAME)\n",
        "if not home:\n",
        "    raise SystemExit(\"Could not determine official homepage. Set OVERRIDE_HOMEPAGE explicitly (e.g., 'https://www.ing.com').\")\n",
        "\n",
        "print(\"üè† Homepage:\", home)\n",
        "raw = discover_candidates(home, max_candidates=MAX_CANDIDATES)\n",
        "\n",
        "# Expand a few hub pages to gather .htm content links\n",
        "if EXPAND_FROM_HUBS:\n",
        "    hub_candidates = [c for c in raw if is_hub_path(urlparse(c[\"url\"]).path)]\n",
        "    hub_candidates = hub_candidates[:MAX_HUBS_TO_EXPAND]\n",
        "    expanded = []\n",
        "    for hc in hub_candidates:\n",
        "        hub_url = cleanup_url(hc[\"url\"])\n",
        "        if not allowed_by_robots(hub_url):\n",
        "            continue\n",
        "        try:\n",
        "            r = fetch(hub_url)\n",
        "            if r.ok and \"html\" in (r.headers.get(\"content-type\",\"\")):\n",
        "                links = extract_samehost_links(r.url, r.text, limit=MAX_LINKS_PER_HUB)\n",
        "                for u in links:\n",
        "                    expanded.append({\"url\": u, \"reason\": \"page_links\", \"boost\": 1.0})\n",
        "        except:\n",
        "            pass\n",
        "    raw.extend(expanded)\n",
        "\n",
        "# score + normalize + validate\n",
        "seen_urls = set()\n",
        "rows = []\n",
        "for c in raw:\n",
        "    u0 = c[\"url\"]\n",
        "    # normalize & fix potential missing .htm only when appropriate\n",
        "    u1 = ensure_htm(cleanup_url(u0))\n",
        "    # de-dupe by normalized URL\n",
        "    if u1 in seen_urls:\n",
        "        continue\n",
        "    seen_urls.add(u1)\n",
        "    ok, final_u, status = preflight(u1)\n",
        "    rows.append({\n",
        "        \"url\": u0,\n",
        "        \"normalized_url\": u1,\n",
        "        \"final_url\": final_u,\n",
        "        \"reason\": c[\"reason\"],\n",
        "        \"score\": score(u1, c[\"reason\"], c[\"boost\"]),\n",
        "        \"status_code\": status,\n",
        "        \"is_live\": bool(ok)\n",
        "    })\n",
        "\n",
        "# Sort: live first, then score\n",
        "df = pd.DataFrame(rows)\n",
        "df.sort_values(by=[\"is_live\",\"score\"], ascending=[False, False], inplace=True)\n",
        "df.to_csv(\"candidates.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
        "\n",
        "# Export top-N live URLs (canonical) for scraping later\n",
        "top_live = df[df[\"is_live\"]].head(TOP_N_LIVE)[\"final_url\"].tolist()\n",
        "with open(\"top17_live.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for u in top_live:\n",
        "        f.write(u + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Discovery complete. {len(df)} candidates saved to candidates.csv\")\n",
        "print(f\"üü¢ Live URLs exported to top17_live.txt ({len(top_live)} URLs)\\n\")\n",
        "\n",
        "print(\"Top 15 live candidates:\")\n",
        "preview = df[df[\"is_live\"]].head(15)[[\"score\",\"reason\",\"final_url\",\"status_code\"]]\n",
        "if preview.empty:\n",
        "    print(\"(No live URLs found among candidates ‚Äî try raising MAX_HUBS_TO_EXPAND or widening KEYWORDS_URL.)\")\n",
        "else:\n",
        "    print(preview.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfXHqx4x--RZ",
        "outputId": "edab92d6-ac77-4515-c669-992a099df35d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/97.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüè† Homepage: https://www.ing.com\n",
            "‚úÖ Discovery complete. 82 candidates saved to candidates.csv\n",
            "üü¢ Live URLs exported to top17_live.txt (17 URLs)\n",
            "\n",
            "Top 15 live candidates:\n",
            " score     reason                                                                                       final_url  status_code\n",
            "  8.20 page_links                                             https://www.ing.com/About-us/Purpose-and-values.htm          200\n",
            "  5.70 page_links                                                https://www.ing.com/About-us/ING-at-a-glance.htm          200\n",
            "  5.70 page_links                                                       https://www.ing.com/About-us/Strategy.htm          200\n",
            "  5.70 page_links                                           https://www.ing.com/About-us/Corporate-governance.htm          200\n",
            "  5.40 page_links            https://www.ing.com/About-us/Corporate-governance/Legal-structure-and-regulators.htm          200\n",
            "  5.40 page_links                     https://www.ing.com/About-us/Corporate-governance/Shareholder-influence.htm          200\n",
            "  5.40 page_links           https://www.ing.com/About-us/Corporate-governance/Dutch-Corporate-Governance-Code.htm          200\n",
            "  5.40 page_links                        https://www.ing.com/About-us/Corporate-governance/Dutch-Banking-Code.htm          200\n",
            "  5.40 page_links                    https://www.ing.com/About-us/Corporate-governance/NYSE-listing-standards.htm          200\n",
            "  5.40 page_links                                  https://www.ing.com/About-us/Corporate-governance/Auditors.htm          200\n",
            "  5.40 page_links                              https://www.ing.com/About-us/Corporate-governance/Remuneration.htm          200\n",
            "  5.40 page_links https://www.ing.com/About-us/Corporate-governance/Supervisory-Public-and-Regulatory-Affairs.htm          200\n",
            "  5.40 page_links                                      https://www.ing.com/About-us/Compliance/Tax-principles.htm          200\n",
            "  5.40 page_links    https://www.ing.com/Sustainability/Partnerships-and-collective-action/Equator-Principles.htm          200\n",
            "  4.89 page_links                                                                https://www.ing.com/About-us.htm          200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ **Batch + Scoring Version**\n",
        "\n",
        "This second code expands the scraper by adding a scoring and labeling system.\n",
        "It doesn‚Äôt just download and clean pages ‚Äî it also identifies and ranks which ones are most relevant to topics like Mission, Vision, Values, Strategy, or Sustainability.\n",
        "\n",
        "What it adds:\n",
        "\n",
        "*  **A keyword-based scoring system:**\n",
        "\n",
        "    *  Each keyword (like ‚Äúmission‚Äù, ‚Äúvision‚Äù,‚Äústrategy‚Äù) has a weight that defines its importance.\n",
        "\n",
        "*  **The program looks for these words in the page‚Äôs URL, title, and headings.**\n",
        "    *  The higher the number and importance of matches, the higher the page‚Äôs score.\n",
        "\n",
        "*  **A labeling system:**\n",
        "\n",
        "    * Each page is automatically tagged with a label (e.g., ‚ÄúMission‚Äù, ‚ÄúVision‚Äù, ‚ÄúStrategy‚Äù, etc.) depending on which keywords appear.\n",
        "\n",
        "*  **Ranking and grouping:**\n",
        "\n",
        "    *   It saves all pages and their scores in best_sites_raw.csv.\n",
        "    *   It then selects the top pages per company (for example, the 5 most relevant pages) and saves them in best_sites_top.csv.\n",
        "\n",
        "\n",
        "üëâ Purpose: This version not only scrapes pages but also analyzes and ranks them to help you quickly find the best pages that contain a company‚Äôs mission, vision, or sustainability statements."
      ],
      "metadata": {
        "id": "JLsNEPzO27is"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw29r6QBzC1l",
        "outputId": "f02e7729-795c-4562-d9c7-1d68bd1da404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 17 URL(s). Output ‚Üí out_md/ and manifest.csv\n",
            "\n",
            "[1/17] (unknown) ‚Äî https://www.ing.com/About-us/Purpose-and-values.htm\n",
            "‚úÖ Saved: out_md/content_purpose-and-values-htm-b75a9602.md (blocks=19)\n",
            "\n",
            "[2/17] (unknown) ‚Äî https://www.ing.com/About-us/ING-at-a-glance.htm\n",
            "‚úÖ Saved: out_md/content_ing-at-a-glance-htm-6fba7b37.md (blocks=13)\n",
            "\n",
            "[3/17] (unknown) ‚Äî https://www.ing.com/About-us/Strategy.htm\n",
            "‚úÖ Saved: out_md/content_strategy-htm-1713d8e3.md (blocks=8)\n",
            "\n",
            "[4/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance.htm\n",
            "‚úÖ Saved: out_md/content_corporate-governance-htm-61ca237d.md (blocks=7)\n",
            "\n",
            "[5/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/Legal-structure-and-regulators.htm\n",
            "‚úÖ Saved: out_md/content_legal-structure-and-regulators-htm-2191c1b7.md (blocks=7)\n",
            "\n",
            "[6/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/Shareholder-influence.htm\n",
            "‚úÖ Saved: out_md/content_shareholder-influence-htm-95dd52b7.md (blocks=10)\n",
            "\n",
            "[7/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/Dutch-Corporate-Governance-Code.htm\n",
            "‚úÖ Saved: out_md/content_dutch-corporate-governance-code-htm-6aa0a71f.md (blocks=4)\n",
            "\n",
            "[8/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/Dutch-Banking-Code.htm\n",
            "‚úÖ Saved: out_md/content_dutch-banking-code-htm-e6e5b3f0.md (blocks=5)\n",
            "\n",
            "[9/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/NYSE-listing-standards.htm\n",
            "‚úÖ Saved: out_md/content_nyse-listing-standards-htm-92b9bee9.md (blocks=4)\n",
            "\n",
            "[10/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/Auditors.htm\n",
            "‚úÖ Saved: out_md/content_auditors-htm-110a5911.md (blocks=10)\n",
            "\n",
            "[11/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/Remuneration.htm\n",
            "‚úÖ Saved: out_md/content_remuneration-htm-4657ab07.md (blocks=4)\n",
            "\n",
            "[12/17] (unknown) ‚Äî https://www.ing.com/About-us/Corporate-governance/Supervisory-Public-and-Regulatory-Affairs.htm\n",
            "‚úÖ Saved: out_md/content_supervisory-public-and-regulatory-affairs-htm-85bc7378.md (blocks=13)\n",
            "\n",
            "[13/17] (unknown) ‚Äî https://www.ing.com/About-us/Compliance/Tax-principles.htm\n",
            "‚úÖ Saved: out_md/content_tax-principles-htm-0002971f.md (blocks=36)\n",
            "\n",
            "[14/17] (unknown) ‚Äî https://www.ing.com/Sustainability/Partnerships-and-collective-action/Equator-Principles.htm\n",
            "‚úÖ Saved: out_md/content_equator-principles-htm-0b4bb0aa.md (blocks=7)\n",
            "\n",
            "[15/17] (unknown) ‚Äî https://www.ing.com/About-us.htm\n",
            "‚úÖ Saved: out_md/content_about-us-htm-eb8af482.md (blocks=23)\n",
            "\n",
            "[16/17] (unknown) ‚Äî https://www.ing.com/About-us/Customer-value.htm\n",
            "‚úÖ Saved: out_md/content_customer-value-htm-54f757e6.md (blocks=12)\n",
            "\n",
            "[17/17] (unknown) ‚Äî https://www.ing.com/About-us/Diversity-inclusion-and-belonging.htm\n",
            "‚úÖ Saved: out_md/content_diversity-inclusion-and-belonging-htm-89a6d595.md (blocks=43)\n",
            "\n",
            "üìÑ Manifest written: manifest.csv\n",
            "‚úì Saved best_sites_raw.csv\n",
            "‚úì Saved best_sites_top.csv (top 5 per company)\n"
          ]
        }
      ],
      "source": [
        "# @title üóÇÔ∏è Batch + Scoring: scrape up to 17 pages, save .md, and rank best pages\n",
        "# @markdown Provide URLs directly in URLS or point to a candidates.csv; outputs out_md/, manifest.csv, best_sites_raw.csv, best_sites_top.csv\n",
        "\n",
        "import os, re, csv, time, hashlib\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, urljoin, urlunparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib import robotparser\n",
        "\n",
        "# ========= Choose input source =========\n",
        "# Option A: paste your chosen URLs here (comment out Option B if you use this):\n",
        "URLS = [\n",
        "    # \"https://www.ing.com/About-us/Purpose-and-values.htm\",\n",
        "    # \"https://www.ing.com/About-us/Strategy.htm\",\n",
        "]\n",
        "\n",
        "# Option B: read from candidates.csv (produced by discovery step)\n",
        "READ_FROM_CSV = True            # set False if using Option A above\n",
        "CANDIDATES_CSV = \"candidates.csv\"\n",
        "CSV_URL_COLUMN = \"url\"          # change if your column is differently named\n",
        "CSV_COMPANY_COLUMN = \"company\"  # optional; if missing, \"(unknown)\" is used\n",
        "TOP_N = 17\n",
        "\n",
        "# ========= Settings =========\n",
        "DELAY_BETWEEN = 1.0             # polite delay between requests (seconds)\n",
        "RETRY_ATTEMPTS = 3              # per-URL fetch attempts\n",
        "BACKOFF_SECONDS = 1.5           # exponential backoff base\n",
        "OUT_DIR = \"out_md\"\n",
        "MANIFEST = \"manifest.csv\"\n",
        "REQUEST_TIMEOUT = 15\n",
        "TOP_PER_COMPANY = 5             # how many \"best\" URLs to keep per company (for best_sites_top.csv)\n",
        "\n",
        "UA = (\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        ")\n",
        "\n",
        "# ========= Scoring model =========\n",
        "KEYWORD_WEIGHTS = {\n",
        "    \"mission\": 10, \"vision\": 10, \"purpose\": 9, \"values\": 9,\n",
        "    \"strategy\": 8, \"sustainability\": 10, \"esg\": 9, \"csr\": 8,\n",
        "    \"about\": 7, \"who-we-are\": 7, \"our-company\": 7, \"company\": 5,\n",
        "    \"annual\": 9, \"report\": 8, \"integrated\": 7, \"governance\": 6,\n",
        "    \"code-of-conduct\": 6, \"principles\": 6, \"culture\": 6, \"impact\": 6,\n",
        "    \"responsibility\": 6, \"investor\": 6, \"purpose-and-values\": 9,\n",
        "}\n",
        "\n",
        "LABEL_RULES = [\n",
        "    (\"mission\", \"Mission\"),\n",
        "    (\"vision\", \"Vision\"),\n",
        "    (\"purpose\", \"Purpose\"),\n",
        "    (\"values\", \"Values\"),\n",
        "    (\"strategy\", \"Strategy\"),\n",
        "    (\"sustainability|esg|csr|responsibility|impact\", \"Sustainability\"),\n",
        "    (r\"\\bannual\\b|\\breport\\b|\\bintegrated\\b|\\binvestor\\b\", \"Annual/Report\"),\n",
        "    (\"about|who-we-are|our-story|our-company|company\", \"About/Company\"),\n",
        "    (\"governance|code-of-conduct|principles\", \"Governance\"),\n",
        "]\n",
        "LABEL_PRIORITY = [\n",
        "    \"Mission\",\"Vision\",\"Values\",\"Purpose\",\"Strategy\",\n",
        "    \"Sustainability\",\"Annual/Report\",\"About/Company\",\"Governance\",\"General\"\n",
        "]\n",
        "LABEL_RANK = {lab: i for i, lab in enumerate(LABEL_PRIORITY)}\n",
        "\n",
        "# ========= Helpers =========\n",
        "\n",
        "# robots.txt cache\n",
        "class RobotsCache:\n",
        "    def __init__(self):\n",
        "        self._cache = {}\n",
        "    def allowed(self, user_agent: str, url: str) -> bool:\n",
        "        netloc = urlparse(url).netloc\n",
        "        if not netloc:\n",
        "            return False\n",
        "        rp = self._cache.get(netloc)\n",
        "        if rp is None:\n",
        "            rp = robotparser.RobotFileParser()\n",
        "            robots_url = f\"https://{netloc}/robots.txt\"\n",
        "            try:\n",
        "                rp.set_url(robots_url)\n",
        "                rp.read()\n",
        "            except Exception:\n",
        "                rp = None\n",
        "            self._cache[netloc] = rp\n",
        "        if self._cache[netloc] is None:\n",
        "            return True\n",
        "        try:\n",
        "            return self._cache[netloc].can_fetch(user_agent, url)\n",
        "        except Exception:\n",
        "            return True\n",
        "\n",
        "ROBOTS = RobotsCache()\n",
        "def allowed_by_robots(url: str) -> bool:\n",
        "    return ROBOTS.allowed(UA, url)\n",
        "\n",
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Normalize URL (drop fragments, ensure scheme).\"\"\"\n",
        "    if not url:\n",
        "        return url\n",
        "    p = urlparse(url)\n",
        "    scheme = p.scheme or \"https\"\n",
        "    return urlunparse((scheme, p.netloc, p.path or \"/\", p.params, p.query, \"\"))\n",
        "\n",
        "def fetch(url: str):\n",
        "    \"\"\"HTTP GET with UA, timeout, and redirects allowed.\"\"\"\n",
        "    return requests.get(url, headers={\"User-Agent\": UA}, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
        "\n",
        "def _slug_from_url(url: str) -> str:\n",
        "    \"\"\"Stable, readable filename slug from URL.\"\"\"\n",
        "    p = urlparse(url)\n",
        "    last = (p.path.rstrip(\"/\").split(\"/\")[-1] or \"home\").lower()\n",
        "    last = re.sub(r\"[^a-z0-9\\-]+\", \"-\", last).strip(\"-\") or \"page\"\n",
        "    h = hashlib.sha1(url.encode(\"utf-8\")).hexdigest()[:8]\n",
        "    return f\"{last}-{h}\"\n",
        "\n",
        "def _retry_fetch(url: str, attempts=RETRY_ATTEMPTS, backoff=BACKOFF_SECONDS):\n",
        "    last_exc = None\n",
        "    for i in range(attempts):\n",
        "        try:\n",
        "            r = fetch(url)\n",
        "            if 200 <= r.status_code < 400:\n",
        "                return r\n",
        "            last_exc = RuntimeError(f\"HTTP {r.status_code}\")\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "        time.sleep(backoff * (2 ** i))\n",
        "    raise last_exc if last_exc else RuntimeError(\"Unknown fetch error\")\n",
        "\n",
        "def _load_urls():\n",
        "    if URLS:\n",
        "        return [{\"url\": normalize_url(u), \"company\": \"(unknown)\"} for u in URLS[:TOP_N]]\n",
        "    if READ_FROM_CSV and os.path.exists(CANDIDATES_CSV):\n",
        "        df = pd.read_csv(CANDIDATES_CSV)\n",
        "        url_col = CSV_URL_COLUMN if CSV_URL_COLUMN in df.columns else df.columns[0]\n",
        "        comp_col = CSV_COMPANY_COLUMN if CSV_COMPANY_COLUMN in df.columns else None\n",
        "        rows = []\n",
        "        seen = set()\n",
        "        for _, r in df.iterrows():\n",
        "            u = normalize_url(str(r[url_col]).strip())\n",
        "            if not u or u in seen:\n",
        "                continue\n",
        "            seen.add(u)\n",
        "            company = str(r[comp_col]).strip() if comp_col else \"(unknown)\"\n",
        "            rows.append({\"url\": u, \"company\": company})\n",
        "            if len(rows) >= TOP_N:\n",
        "                break\n",
        "        return rows\n",
        "    raise SystemExit(\"No URLs provided. Fill URLS list or supply candidates.csv.\")\n",
        "\n",
        "# ---- Content cleaning / block extraction ----\n",
        "def strip_noncontent(soup: BeautifulSoup) -> None:\n",
        "    \"\"\"In-place removal of boilerplate to keep meaningful text.\"\"\"\n",
        "    for sel in [\n",
        "        \"nav\", \"header\", \"footer\",\n",
        "        \"[role='banner']\", \"[role='navigation']\", \"[role='contentinfo']\",\n",
        "        \".cookie\", \"#cookie\", \"[id*='cookie']\", \"[class*='cookie']\",\n",
        "        \".consent\", \"[id*='consent']\",\n",
        "        \".newsletter\", \".subscribe\", \".social\", \".share\",\n",
        "        \"script\", \"style\", \"noscript\", \"svg\"\n",
        "    ]:\n",
        "        for el in soup.select(sel):\n",
        "            el.decompose()\n",
        "\n",
        "def _clean_text(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", t or \"\").strip()\n",
        "\n",
        "def collect_blocks(soup: BeautifulSoup):\n",
        "    \"\"\"Collect headings/paragraphs in reading order.\"\"\"\n",
        "    blocks = []\n",
        "    main = soup.select_one(\"main\") or soup.body or soup\n",
        "    for el in main.find_all([\"h1\",\"h2\",\"h3\",\"p\"], recursive=True):\n",
        "        txt = _clean_text(el.get_text(\" \", strip=True))\n",
        "        if not txt:\n",
        "            continue\n",
        "        blocks.append({\"type\": el.name.lower(), \"text\": txt})\n",
        "    if not blocks:\n",
        "        for el in soup.find_all([\"article\",\"p\"]):\n",
        "            txt = _clean_text(el.get_text(\" \", strip=True))\n",
        "            if txt:\n",
        "                blocks.append({\"type\": el.name.lower(), \"text\": txt})\n",
        "    return blocks\n",
        "\n",
        "def to_md(blocks):\n",
        "    lines = []\n",
        "    for b in blocks:\n",
        "        t, x = b[\"type\"], b[\"text\"]\n",
        "        if t == \"h1\": lines.append(f\"# {x}\")\n",
        "        elif t == \"h2\": lines.append(f\"## {x}\")\n",
        "        elif t == \"h3\": lines.append(f\"### {x}\")\n",
        "        else: lines.append(x)\n",
        "        lines.append(\"\")\n",
        "    return \"\\n\".join(lines).strip() + \"\\n\"\n",
        "\n",
        "# ========= Scoring helpers =========\n",
        "def _score_blob(text: str) -> int:\n",
        "    text = (text or \"\").lower()\n",
        "    return sum(w for kw, w in KEYWORD_WEIGHTS.items() if kw in text)\n",
        "\n",
        "def _label_for(url: str, title: str, h1: str, h2: str) -> str:\n",
        "    blob = \" \".join(x for x in [url, title, h1, h2] if x).lower()\n",
        "    for pat, lab in LABEL_RULES:\n",
        "        if re.search(pat, blob):\n",
        "            return lab\n",
        "    return \"General\"\n",
        "\n",
        "def score_page(url: str, html: str):\n",
        "    \"\"\"Return dict with url/title/h1/h2/label/score.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    strip_noncontent(soup)\n",
        "    title = soup.title.get_text(\" \", strip=True) if soup.title else \"\"\n",
        "    h1_el = soup.select_one(\"h1\")\n",
        "    h2_el = soup.select_one(\"h2\")\n",
        "    h1 = h1_el.get_text(\" \", strip=True) if h1_el else \"\"\n",
        "    h2 = h2_el.get_text(\" \", strip=True) if h2_el else \"\"\n",
        "\n",
        "    score = 0\n",
        "    score += 2 * _score_blob(url)\n",
        "    score += 2 * _score_blob(title)\n",
        "    score += _score_blob(h1)\n",
        "    score += _score_blob(h2)\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"h1\": h1,\n",
        "        \"h2\": h2,\n",
        "        \"label\": _label_for(url, title, h1, h2),\n",
        "        \"score\": score,\n",
        "    }\n",
        "\n",
        "# ---- run ----\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "targets = _load_urls()\n",
        "print(f\"Processing {len(targets)} URL(s). Output ‚Üí {OUT_DIR}/ and {MANIFEST}\")\n",
        "\n",
        "rows_manifest = []\n",
        "rows_scored = []\n",
        "\n",
        "for idx, item in enumerate(targets, 1):\n",
        "    url = item[\"url\"]\n",
        "    company = item[\"company\"]\n",
        "    print(f\"\\n[{idx}/{len(targets)}] {company} ‚Äî {url}\")\n",
        "    try:\n",
        "        if not allowed_by_robots(url):\n",
        "            print(\"‚õî Blocked by robots.txt\")\n",
        "            rows_manifest.append({\"company\": company, \"url\": url, \"status\": \"robots_blocked\", \"md_file\": \"\", \"blocks\": 0, \"snippet\": \"\"})\n",
        "            continue\n",
        "\n",
        "        resp = _retry_fetch(url)\n",
        "        ctype = resp.headers.get(\"content-type\",\"\").lower()\n",
        "        if \"html\" not in ctype:\n",
        "            print(f\"‚ÑπÔ∏è Skipping non-HTML content: {ctype}\")\n",
        "            rows_manifest.append({\"company\": company, \"url\": url, \"status\": \"non_html\", \"md_file\": \"\", \"blocks\": 0, \"snippet\": \"\"})\n",
        "            time.sleep(DELAY_BETWEEN)\n",
        "            continue\n",
        "\n",
        "        # score\n",
        "        scored = score_page(resp.url, resp.text)\n",
        "        scored[\"company\"] = company\n",
        "        rows_scored.append(scored)\n",
        "\n",
        "        # collect blocks + write .md (same as before)\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "        strip_noncontent(soup)\n",
        "        blocks = collect_blocks(soup)\n",
        "\n",
        "        if not blocks:\n",
        "            print(\"‚ÑπÔ∏è No meaningful blocks found.\")\n",
        "            rows_manifest.append({\"company\": company, \"url\": resp.url, \"status\": \"no_blocks\", \"md_file\": \"\", \"blocks\": 0, \"snippet\": \"\"})\n",
        "            time.sleep(DELAY_BETWEEN)\n",
        "            continue\n",
        "\n",
        "        md = to_md(blocks)\n",
        "        fname = f\"{OUT_DIR}/content_{_slug_from_url(resp.url)}.md\"\n",
        "        with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(md)\n",
        "\n",
        "        snippet = (blocks[0][\"text\"][:220] + \"‚Ä¶\") if blocks and blocks[0].get(\"text\") else \"\"\n",
        "        rows_manifest.append({\"company\": company, \"url\": resp.url, \"status\": \"ok\", \"md_file\": fname, \"blocks\": len(blocks), \"snippet\": snippet})\n",
        "        print(f\"‚úÖ Saved: {fname} (blocks={len(blocks)})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        rows_manifest.append({\"company\": company, \"url\": url, \"status\": f\"error: {type(e).__name__}\", \"md_file\": \"\", \"blocks\": 0, \"snippet\": \"\"})\n",
        "\n",
        "    time.sleep(DELAY_BETWEEN)\n",
        "\n",
        "# Write manifest\n",
        "pd.DataFrame(rows_manifest, columns=[\"company\",\"url\",\"status\",\"md_file\",\"blocks\",\"snippet\"]).to_csv(MANIFEST, index=False, quoting=csv.QUOTE_ALL)\n",
        "print(f\"\\nüìÑ Manifest written: {MANIFEST}\")\n",
        "\n",
        "# Write scored raw\n",
        "if rows_scored:\n",
        "    raw_df = pd.DataFrame(rows_scored).drop_duplicates([\"company\",\"url\"])\n",
        "    raw_df.to_csv(\"best_sites_raw.csv\", index=False)\n",
        "    print(\"‚úì Saved best_sites_raw.csv\")\n",
        "\n",
        "    # Rank: label priority first, then score (desc)\n",
        "    raw_df[\"label_rank\"] = raw_df[\"label\"].map(LABEL_RANK).fillna(len(LABEL_RANK))\n",
        "    raw_df = raw_df.sort_values([\"company\",\"label_rank\",\"score\"], ascending=[True, True, False])\n",
        "\n",
        "    # Grouped top-K (if no company column, all items will be under '(unknown)')\n",
        "    best = raw_df.groupby(\"company\").head(TOP_PER_COMPANY)\n",
        "    best.to_csv(\"best_sites_top.csv\", index=False)\n",
        "    print(f\"‚úì Saved best_sites_top.csv (top {TOP_PER_COMPANY} per company)\")\n",
        "else:\n",
        "    print(\"No scored pages; best_sites_* files not written.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert ipynb to HTML in Colab\n",
        "# Upload ipynb\n",
        "from google.colab import files\n",
        "f = files.upload()\n",
        "\n",
        "# Convert ipynb to html\n",
        "import subprocess\n",
        "file0 = list(f.keys())[0]\n",
        "_ = subprocess.run([\"pip\", \"install\", \"nbconvert\"])\n",
        "_ = subprocess.run([\"jupyter\", \"nbconvert\", file0, \"--to\", \"html\"])\n",
        "\n",
        "# download the html\n",
        "files.download(file0[:-5]+\"html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "hCN0Ri4sn6oc",
        "outputId": "0784bbd3-c280-4016-87c2-89620c5bcb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ffaf89c6-567e-47da-97f8-7452bc9239dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ffaf89c6-567e-47da-97f8-7452bc9239dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WebScrappingWorking_2.ipynb to WebScrappingWorking_2.ipynb\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_904fcbab-a412-4019-bc0b-06a336680ae9\", \"WebScrappingWorking_2.html\", 373634)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}